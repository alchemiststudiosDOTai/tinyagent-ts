// src/agent.ts
import "reflect-metadata";
import { ZodError } from "zod"; // Removed unused 'z' and 'ZodSchema'
import { META_KEYS, ToolMetadata } from "./decorators";
import { PromptEngine } from "./promptEngine";
import { FinalAnswerTool } from "./final-answer.tool";
import { AssistantReplySchema } from "./schemas";
import { z } from "zod";

/**
 * Represents a tool's runtime information, including its metadata and
 * a callable function to execute it.
 * @internal
 */
interface ToolHandle {
  /** The metadata associated with the tool, as defined by the `@tool` decorator. */
  meta: ToolMetadata;
  /**
   * An asynchronous function that executes the tool's logic.
   * @param args - The arguments to pass to the tool, expected to conform to `meta.schema`.
   * @returns A promise that resolves with the result of the tool execution.
   */
  call: (args: Record<string, unknown>) => Promise<unknown>;
}

/**
 * Defines the expected structure of a successful response from the OpenRouter API.
 * @internal
 */
interface OpenRouterResponse {
  /** An array of choices, typically containing one primary response. */
  choices: Array<{
    /** The message object containing the content generated by the LLM. */
    message: {
      /** The textual content of the LLM's response. */
      content: string;
    };
  }>;
}

/**
 * Defines the structure for messages sent to the LLM.
 * @internal
 */
export interface LLMMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

/**
 * Abstract base class for creating AI agents.
 * Agents can be equipped with tools (defined by `@tool` decorator) and use an LLM
 * (specified by `@model` decorator) to process input and decide whether to use a tool
 * or respond directly.
 *
 * @template I - The type of the input the agent's `run` method accepts. Defaults to `string`.
 * @template O - The type of the output the agent's `run` method produces. Defaults to `string`.
 */
export abstract class Agent<I = string, O = string> {
  /** The API key for OpenRouter, loaded from environment variables. */
  private readonly apiKey: string;
  protected readonly customSystemPrompt?: string;
  protected readonly promptEngine: PromptEngine;
  /** Conversation memory for ReAct loop */
  protected readonly memory: LLMMessage[] = [];

  /**
   * Initializes a new instance of the Agent.
   * It requires the `OPENROUTER_API_KEY` environment variable to be set.
   * @throws Error if `OPENROUTER_API_KEY` is not found in the environment variables.
   */
  constructor(options: { systemPrompt?: string; systemPromptFile?: string } = {}) {
    const { systemPrompt, systemPromptFile } = options;

    const apiKey = process.env.OPENROUTER_API_KEY;
    if (!apiKey) {
      // TODO: Replace with AgentInitializationError
      throw new Error("OPENROUTER_API_KEY environment variable is required");
    }
    this.apiKey = apiKey;
    this.customSystemPrompt = systemPrompt;
    this.promptEngine = new PromptEngine({}, systemPromptFile ? { agent: systemPromptFile } : {});
  }

  /**
   * Retrieves the LLM model name associated with this agent class.
   * The model name is specified using the `@model` decorator.
   * @returns The model name string.
   * @throws Error if the `@model` decorator is missing on the agent class.
   * @internal
   */
  protected getModelName(): string {
    const id: string | undefined = Reflect.getMetadata(
      META_KEYS.MODEL,
      this.constructor,
    );
    if (!id) {
      // TODO: Replace with AgentConfigurationError
      throw new Error("Missing @model decorator on the Agent class.");
    }
    return id;
  }

  /**
   * Builds a registry of tools available to this agent.
   * Tools are defined using the `@tool` decorator on methods of the agent class.
   * @returns A record mapping tool names to their `ToolHandle` (metadata and call function).
   * @internal
   */
  protected buildToolRegistry(): Record<string, ToolHandle> {
    const metaList: ToolMetadata[] =
      Reflect.getMetadata(META_KEYS.TOOLS, this.constructor) || [];
    return Object.fromEntries(
      metaList.map((m) => [
        m.name,
        {
          meta: m,
          call: async (args: Record<string, unknown>): Promise<unknown> => {
            try {
              m.schema.parse(args); // Validate arguments using Zod schema
            } catch (error) {
              if (error instanceof ZodError) {
                // TODO: Replace with ToolValidationError
                throw new Error(
                  `Invalid arguments for tool "${m.name}": ${error.errors.map((e) => `${e.path.join(".")}: ${e.message}`).join(", ")}`,
                );
              }
              throw error; // Re-throw unexpected errors
            }
            try {
              // Dynamically call the method associated with the tool
              return await (this as any)[m.method](args);
            } catch (error) {
              // TODO: Replace with ToolExecutionError
              const message =
                error instanceof Error ? error.message : String(error);
              throw new Error(`Error executing tool "${m.name}": ${message}`);
            }
          },
        },
      ]),
    );
  }

  /**
   * Makes a request to the OpenRouter API.
   * @param messages - An array of message objects to send to the LLM.
   * @param model - The name of the LLM model to use.
   * @returns A promise that resolves with the API response.
   * @throws Error if the API request fails or returns an error status.
   * @internal
   */
  protected async makeOpenRouterRequest(
    messages: LLMMessage[],
    model: string,
  ): Promise<OpenRouterResponse> {
    try {
      const res = await fetch("https://openrouter.ai/api/v1/chat/completions", {
        method: "POST",
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          "Content-Type": "application/json",
          // TODO: Make these configurable or remove/improve defaults
          "HTTP-Referer": "https://github.com/yourusername/tinyagent-ts",
          "X-Title": "TinyAgent-TS",
        },
        body: JSON.stringify({ model, messages }),
      });

      if (!res.ok) {
        let errorDetails: any = { message: "Failed to parse error response" };
        try {
          errorDetails = await res.json();
        } catch (parseError) {
          // Ignore parsing error, use default message
        }
        // TODO: Replace with LLMCommunicationError
        throw new Error(
          `OpenRouter API error: ${res.status} ${res.statusText}. Details: ${JSON.stringify(errorDetails)}`,
        );
      }

      // Explicitly type data after parsing JSON
      const data: any = await res.json();

      // Perform type checks before accessing properties
      if (
        !data ||
        typeof data !== "object" ||
        !data.choices ||
        !Array.isArray(data.choices) ||
        data.choices.length === 0 ||
        typeof data.choices[0]?.message?.content !== "string" // Check nested structure
      ) {
        console.error(
          "Invalid OpenRouter response structure:",
          JSON.stringify(data),
        );
        throw new Error(
          "Invalid response structure received from OpenRouter API",
        );
      }
      // Now it's safer to assert the type
      return data as OpenRouterResponse;
    } catch (error: unknown) {
      const message = error instanceof Error ? error.message : String(error);
      // TODO: Replace with LLMCommunicationError
      throw new Error(`Failed to call OpenRouter API: ${message}`);
    }
  }

  /**
   * Main entry point for running the agent.
   * It processes the input, interacts with the LLM, and potentially uses tools
   * to generate a final output.
   * @param input - The input to be processed by the agent.
   * @returns A promise that resolves with the agent's final output.
   */
  async run(input: I): Promise<O> {
    const modelName = this.getModelName();
    const tools = this.buildToolRegistry();
    const toolCatalog = Object.values(tools)
      .map((t) => `- ${t.meta.name}: ${t.meta.description}`)
      .join("\n");

    // Use new helper to build initial messages
    if (this.memory.length === 0) {
      const initialMessages = this.buildInitialMessages(input, toolCatalog);
      for (const msg of initialMessages) {
        this.memory.push(msg);
      }
    } else {
      this.memory.push({ role: "user", content: String(input) });
    }

    const finalTool = new FinalAnswerTool();
    const maxSteps = 5;

    let lastValidReply: any = null;

    for (let step = 0; step < maxSteps; step++) {
      const responseBody = await this.makeOpenRouterRequest(this.memory, modelName);
      const rawReply = responseBody.choices[0]?.message?.content?.trim() ?? "";
      let parsed: any;
      let validation = null;

      try {
        parsed = JSON.parse(rawReply);
        validation = AssistantReplySchema.safeParse(parsed);
      } catch (err) {
        validation = { success: false, error: new Error("Invalid JSON") };
      }

      if (!validation || !validation.success) {
        // Retry with fix request
        const { fixed, fixedParsed } = await this.retryWithFixRequest(rawReply, validation?.error);
        if (!fixedParsed) {
          // If still invalid, return the last valid reply or error
          this.memory.push({ role: "assistant", content: `ERROR: Unable to produce valid schema output.` });
          return fixed as unknown as O;
        }
        parsed = fixedParsed;
        this.memory.push({ role: "assistant", content: JSON.stringify(parsed) });
      } else {
        this.memory.push({ role: "assistant", content: rawReply });
      }

      // At this point, parsed is valid per schema
      lastValidReply = parsed;

      if ("tool" in parsed) {
        const toolName = parsed.tool;
        const toolArgs = parsed.args;

        if (toolName === finalTool.name) {
          const answer = await finalTool.forward(toolArgs);
          return answer as O;
        }

        const selectedTool = tools[toolName];

        if (!selectedTool) {
          const errorMsg = `Error: LLM requested unknown tool "${toolName}". Available tools: ${Object.keys(tools).join(", ")}`;
          this.memory.push({ role: "assistant", content: errorMsg });
          return errorMsg as unknown as O;
        }

        let toolResult;
        try {
          toolResult = await selectedTool.call(toolArgs);
        } catch (error) {
          if (error instanceof z.ZodError) {
            // Retry with fix request for tool arguments
            const { fixed, fixedParsed } = await this.retryWithFixRequest(
              JSON.stringify({ tool: toolName, args: toolArgs }),
              error
            );
            if (!fixedParsed || !("tool" in fixedParsed) || fixedParsed.tool !== toolName) {
              this.memory.push({ role: "assistant", content: `ERROR: Unable to produce valid tool arguments for "${toolName}".` });
              return fixed as unknown as O;
            }
            try {
              toolResult = await selectedTool.call(fixedParsed.args);
            } catch (err2) {
              this.memory.push({ role: "assistant", content: `ERROR: Tool argument validation failed again for "${toolName}".` });
              return (err2 instanceof Error ? err2.message : String(err2)) as unknown as O;
            }
            this.memory.push({ role: "assistant", content: `TOOL_RESULT: ${JSON.stringify(toolResult)}` });
            continue; // Next loop iteration with updated memory
          } else {
            this.memory.push({ role: "assistant", content: `ERROR: Tool execution failed for "${toolName}": ${error instanceof Error ? error.message : String(error)}` });
            return (error instanceof Error ? error.message : String(error)) as unknown as O;
          }
        }
        this.memory.push({ role: "assistant", content: `TOOL_RESULT: ${JSON.stringify(toolResult)}` });
        continue; // Next loop iteration with updated memory
      } else if ("answer" in parsed) {
        console.log("[Agent.run] Returning parsed object (with answer):", parsed, "Type:", typeof parsed);
        return parsed as unknown as O;
      } else {
        // Should not happen due to schema, but fallback
        console.log("[Agent.run] Fallback: Returning stringified parsed object:", parsed, "Type:", typeof parsed);
        this.memory.push({ role: "assistant", content: `ERROR: Unexpected schema output.` });
        return parsed as unknown as O;
      }
    }

    // Step limit reached
    if (lastValidReply) {
      console.log("[Agent.run] Step limit reached. Returning lastValidReply as object:", lastValidReply, "Type:", typeof lastValidReply);
      return lastValidReply as unknown as O;
    } else {
      console.log("[Agent.run] Step limit reached. Returning last memory content:", this.memory[this.memory.length - 1]?.content, "Type:", typeof this.memory[this.memory.length - 1]?.content);
      return this.memory[this.memory.length - 1]?.content as unknown as O;
    }
  }
  /**
   * Helper to build the initial LLM messages (system + user).
   */
  private buildInitialMessages(input: I, toolCatalog: string): LLMMessage[] {
    const defaultPrompt = this.promptEngine.render("agent", { tools: toolCatalog });
    const systemPrompt = this.customSystemPrompt ?? defaultPrompt;
    return [
      { role: "system" as const, content: systemPrompt },
      { role: "user" as const, content: String(input) }
    ];
  }

  /**
   * Helper to retry LLM output with a fix request if schema validation fails.
   * Prompts the LLM to correct its output to match the AssistantReplySchema.
   */
  private async retryWithFixRequest(rawReply: string, error: unknown): Promise<{ fixed: string, fixedParsed: any }> {
    // Compose a retry prompt
    const schemaString = AssistantReplySchema.toString();
    const errorMsg = error instanceof z.ZodError ? error.toString() : String(error);
    const retryPrompt: LLMMessage[] = [
      { role: "system", content: "Your previous response did not match the required schema. Please fix your output to match the following schema exactly:\n\n" + schemaString + "\n\nValidation error:\n" + errorMsg },
      { role: "user", content: rawReply }
    ];
    // Call LLM with retry prompt
    const modelName = this.getModelName();
    const responseBody = await this.makeOpenRouterRequest(retryPrompt, modelName);
    const fixed = responseBody.choices[0]?.message?.content?.trim() ?? "";
    let fixedParsed: any = null;
    try {
      fixedParsed = JSON.parse(fixed);
      const validation = AssistantReplySchema.safeParse(fixedParsed);
      if (!validation.success) {
        fixedParsed = null;
      }
    } catch {
      fixedParsed = null;
    }
    return { fixed, fixedParsed };
  }
}
