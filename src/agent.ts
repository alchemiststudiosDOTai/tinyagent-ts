// src/agent.ts
import "reflect-metadata";
import { ZodError } from "zod"; // Removed unused 'z' and 'ZodSchema'
import { META_KEYS, ToolMetadata } from "./decorators";
import { PromptEngine } from "./promptEngine";
import { FinalAnswerTool } from "./final-answer.tool";

/**
 * Represents a tool's runtime information, including its metadata and
 * a callable function to execute it.
 * @internal
 */
interface ToolHandle {
  /** The metadata associated with the tool, as defined by the `@tool` decorator. */
  meta: ToolMetadata;
  /**
   * An asynchronous function that executes the tool's logic.
   * @param args - The arguments to pass to the tool, expected to conform to `meta.schema`.
   * @returns A promise that resolves with the result of the tool execution.
   */
  call: (args: Record<string, unknown>) => Promise<unknown>;
}

/**
 * Defines the expected structure of a successful response from the OpenRouter API.
 * @internal
 */
interface OpenRouterResponse {
  /** An array of choices, typically containing one primary response. */
  choices: Array<{
    /** The message object containing the content generated by the LLM. */
    message: {
      /** The textual content of the LLM's response. */
      content: string;
    };
  }>;
}

/**
 * Defines the structure for messages sent to the LLM.
 * @internal
 */
export interface LLMMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

/**
 * Abstract base class for creating AI agents.
 * Agents can be equipped with tools (defined by `@tool` decorator) and use an LLM
 * (specified by `@model` decorator) to process input and decide whether to use a tool
 * or respond directly.
 *
 * @template I - The type of the input the agent's `run` method accepts. Defaults to `string`.
 * @template O - The type of the output the agent's `run` method produces. Defaults to `string`.
 */
export abstract class Agent<I = string, O = string> {
  /** The API key for OpenRouter, loaded from environment variables. */
  private readonly apiKey: string;
  private readonly customSystemPrompt?: string;
  protected readonly promptEngine: PromptEngine;
  /** Conversation memory for ReAct loop */
  protected readonly memory: LLMMessage[] = [];

  /**
   * Initializes a new instance of the Agent.
   * It requires the `OPENROUTER_API_KEY` environment variable to be set.
   * @throws Error if `OPENROUTER_API_KEY` is not found in the environment variables.
   */
  constructor(options: { systemPrompt?: string; systemPromptFile?: string } = {}) {
    const { systemPrompt, systemPromptFile } = options;

    const apiKey = process.env.OPENROUTER_API_KEY;
    if (!apiKey) {
      // TODO: Replace with AgentInitializationError
      throw new Error("OPENROUTER_API_KEY environment variable is required");
    }
    this.apiKey = apiKey;
    this.customSystemPrompt = systemPrompt;
    this.promptEngine = new PromptEngine({}, systemPromptFile ? { agent: systemPromptFile } : {});
  }

  /**
   * Retrieves the LLM model name associated with this agent class.
   * The model name is specified using the `@model` decorator.
   * @returns The model name string.
   * @throws Error if the `@model` decorator is missing on the agent class.
   * @internal
   */
  private getModelName(): string {
    const id: string | undefined = Reflect.getMetadata(
      META_KEYS.MODEL,
      this.constructor,
    );
    if (!id) {
      // TODO: Replace with AgentConfigurationError
      throw new Error("Missing @model decorator on the Agent class.");
    }
    return id;
  }

  /**
   * Builds a registry of tools available to this agent.
   * Tools are defined using the `@tool` decorator on methods of the agent class.
   * @returns A record mapping tool names to their `ToolHandle` (metadata and call function).
   * @internal
   */
  private buildToolRegistry(): Record<string, ToolHandle> {
    const metaList: ToolMetadata[] =
      Reflect.getMetadata(META_KEYS.TOOLS, this.constructor) || [];
    return Object.fromEntries(
      metaList.map((m) => [
        m.name,
        {
          meta: m,
          call: async (args: Record<string, unknown>): Promise<unknown> => {
            try {
              m.schema.parse(args); // Validate arguments using Zod schema
            } catch (error) {
              if (error instanceof ZodError) {
                // TODO: Replace with ToolValidationError
                throw new Error(
                  `Invalid arguments for tool "${m.name}": ${error.errors.map((e) => `${e.path.join(".")}: ${e.message}`).join(", ")}`,
                );
              }
              throw error; // Re-throw unexpected errors
            }
            try {
              // Dynamically call the method associated with the tool
              return await (this as any)[m.method](args);
            } catch (error) {
              // TODO: Replace with ToolExecutionError
              const message =
                error instanceof Error ? error.message : String(error);
              throw new Error(`Error executing tool "${m.name}": ${message}`);
            }
          },
        },
      ]),
    );
  }

  /**
   * Makes a request to the OpenRouter API.
   * @param messages - An array of message objects to send to the LLM.
   * @param model - The name of the LLM model to use.
   * @returns A promise that resolves with the API response.
   * @throws Error if the API request fails or returns an error status.
   * @internal
   */
  private async makeOpenRouterRequest(
    messages: LLMMessage[],
    model: string,
  ): Promise<OpenRouterResponse> {
    try {
      const res = await fetch("https://openrouter.ai/api/v1/chat/completions", {
        method: "POST",
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          "Content-Type": "application/json",
          // TODO: Make these configurable or remove/improve defaults
          "HTTP-Referer": "https://github.com/yourusername/tinyagent-ts",
          "X-Title": "TinyAgent-TS",
        },
        body: JSON.stringify({ model, messages }),
      });

      if (!res.ok) {
        let errorDetails: any = { message: "Failed to parse error response" };
        try {
          errorDetails = await res.json();
        } catch (parseError) {
          // Ignore parsing error, use default message
        }
        // TODO: Replace with LLMCommunicationError
        throw new Error(
          `OpenRouter API error: ${res.status} ${res.statusText}. Details: ${JSON.stringify(errorDetails)}`,
        );
      }

      // Explicitly type data after parsing JSON
      const data: any = await res.json();

      // Perform type checks before accessing properties
      if (
        !data ||
        typeof data !== "object" ||
        !data.choices ||
        !Array.isArray(data.choices) ||
        data.choices.length === 0 ||
        typeof data.choices[0]?.message?.content !== "string" // Check nested structure
      ) {
        console.error(
          "Invalid OpenRouter response structure:",
          JSON.stringify(data),
        );
        throw new Error(
          "Invalid response structure received from OpenRouter API",
        );
      }
      // Now it's safer to assert the type
      return data as OpenRouterResponse;
    } catch (error: unknown) {
      const message = error instanceof Error ? error.message : String(error);
      // TODO: Replace with LLMCommunicationError
      throw new Error(`Failed to call OpenRouter API: ${message}`);
    }
  }

  /**
   * Main entry point for running the agent.
   * It processes the input, interacts with the LLM, and potentially uses tools
   * to generate a final output.
   * @param input - The input to be processed by the agent.
   * @returns A promise that resolves with the agent's final output.
   */
  async run(input: I): Promise<O> {
    const modelName = this.getModelName();
    const tools = this.buildToolRegistry();
    const toolCatalog = Object.values(tools)
      .map((t) => `- ${t.meta.name}: ${t.meta.description}`)
      .join("\n");

    const defaultPrompt = this.promptEngine.render("agent", { tools: toolCatalog });
    const systemPrompt = this.customSystemPrompt ?? defaultPrompt;

    if (this.memory.length === 0) {
      this.memory.push({ role: "system", content: systemPrompt });
    }

    this.memory.push({ role: "user", content: String(input) });

    const finalTool = new FinalAnswerTool();
    const maxSteps = 5;

    for (let step = 0; step < maxSteps; step++) {
      const responseBody = await this.makeOpenRouterRequest(this.memory, modelName);
      const reply = responseBody.choices[0]?.message?.content?.trim() ?? "";
      this.memory.push({ role: "assistant", content: reply });

      if (
        reply.startsWith("{") &&
        reply.endsWith("}") &&
        reply.includes('"tool"')
      ) {
        try {
          const parsedJson = JSON.parse(reply);
          if (
            typeof parsedJson.tool !== "string" ||
            typeof parsedJson.args !== "object" ||
            parsedJson.args === null
          ) {
            throw new Error("Invalid tool call format in LLM response.");
          }
          const { tool: toolName, args: toolArgs } = parsedJson as {
            tool: string;
            args: Record<string, unknown>;
          };

          if (toolName === finalTool.name) {
            const answer = await finalTool.forward(toolArgs);
            return answer as O;
          }

          const selectedTool = tools[toolName];

          if (!selectedTool) {
            const errorMsg = `Error: LLM requested unknown tool "${toolName}". Available tools: ${Object.keys(tools).join(", ")}`;
            this.memory.push({ role: "assistant", content: errorMsg });
            return errorMsg as unknown as O;
          }

          const toolResult = await selectedTool.call(toolArgs);
          this.memory.push({ role: "assistant", content: `TOOL_RESULT: ${JSON.stringify(toolResult)}` });
          continue; // Next loop iteration with updated memory
        } catch (error: unknown) {
          const message = error instanceof Error ? error.message : String(error);
          this.memory.push({ role: "assistant", content: `ERROR: ${message}` });
          return reply as unknown as O;
        }
      }

      return reply as unknown as O;
    }

    // Step limit reached
    return this.memory[this.memory.length - 1]?.content as unknown as O;
  }
}
