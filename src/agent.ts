// src/agent.ts
import "reflect-metadata";
import { ZodError } from "zod"; // Removed unused 'z' and 'ZodSchema'
import { META_KEYS, ToolMetadata } from "./decorators";

/**
 * Represents a tool's runtime information, including its metadata and
 * a callable function to execute it.
 * @internal
 */
interface ToolHandle {
  /** The metadata associated with the tool, as defined by the `@tool` decorator. */
  meta: ToolMetadata;
  /**
   * An asynchronous function that executes the tool's logic.
   * @param args - The arguments to pass to the tool, expected to conform to `meta.schema`.
   * @returns A promise that resolves with the result of the tool execution.
   */
  call: (args: Record<string, unknown>) => Promise<unknown>;
}

/**
 * Defines the expected structure of a successful response from the OpenRouter API.
 * @internal
 */
interface OpenRouterResponse {
  /** An array of choices, typically containing one primary response. */
  choices: Array<{
    /** The message object containing the content generated by the LLM. */
    message: {
      /** The textual content of the LLM's response. */
      content: string;
    };
  }>;
}

/**
 * Defines the structure for messages sent to the LLM.
 * @internal
 */
interface LLMMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

/**
 * Abstract base class for creating AI agents.
 * Agents can be equipped with tools (defined by `@tool` decorator) and use an LLM
 * (specified by `@model` decorator) to process input and decide whether to use a tool
 * or respond directly.
 *
 * @template I - The type of the input the agent's `run` method accepts. Defaults to `string`.
 * @template O - The type of the output the agent's `run` method produces. Defaults to `string`.
 */
export abstract class Agent<I = string, O = string> {
  /** The API key for OpenRouter, loaded from environment variables. */
  private readonly apiKey: string;
  private readonly customSystemPrompt?: string;

  /**
   * Initializes a new instance of the Agent.
   * It requires the `OPENROUTER_API_KEY` environment variable to be set.
   * @throws Error if `OPENROUTER_API_KEY` is not found in the environment variables.
   */
  constructor(systemPrompt?: string) {
    const apiKey = process.env.OPENROUTER_API_KEY;
    if (!apiKey) {
      // TODO: Replace with AgentInitializationError
      throw new Error("OPENROUTER_API_KEY environment variable is required");
    }
    this.apiKey = apiKey;
    this.customSystemPrompt = systemPrompt;
  }

  /**
   * Retrieves the LLM model name associated with this agent class.
   * The model name is specified using the `@model` decorator.
   * @returns The model name string.
   * @throws Error if the `@model` decorator is missing on the agent class.
   * @internal
   */
  private getModelName(): string {
    const id: string | undefined = Reflect.getMetadata(
      META_KEYS.MODEL,
      this.constructor,
    );
    if (!id) {
      // TODO: Replace with AgentConfigurationError
      throw new Error("Missing @model decorator on the Agent class.");
    }
    return id;
  }

  /**
   * Builds a registry of tools available to this agent.
   * Tools are defined using the `@tool` decorator on methods of the agent class.
   * @returns A record mapping tool names to their `ToolHandle` (metadata and call function).
   * @internal
   */
  private buildToolRegistry(): Record<string, ToolHandle> {
    const metaList: ToolMetadata[] =
      Reflect.getMetadata(META_KEYS.TOOLS, this.constructor) || [];
    return Object.fromEntries(
      metaList.map((m) => [
        m.name,
        {
          meta: m,
          call: async (args: Record<string, unknown>): Promise<unknown> => {
            try {
              m.schema.parse(args); // Validate arguments using Zod schema
            } catch (error) {
              if (error instanceof ZodError) {
                // TODO: Replace with ToolValidationError
                throw new Error(
                  `Invalid arguments for tool "${m.name}": ${error.errors.map((e) => `${e.path.join(".")}: ${e.message}`).join(", ")}`,
                );
              }
              throw error; // Re-throw unexpected errors
            }
            try {
              // Dynamically call the method associated with the tool
              return await (this as any)[m.method](args);
            } catch (error) {
              // TODO: Replace with ToolExecutionError
              const message =
                error instanceof Error ? error.message : String(error);
              throw new Error(`Error executing tool "${m.name}": ${message}`);
            }
          },
        },
      ]),
    );
  }

  /**
   * Makes a request to the OpenRouter API.
   * @param messages - An array of message objects to send to the LLM.
   * @param model - The name of the LLM model to use.
   * @returns A promise that resolves with the API response.
   * @throws Error if the API request fails or returns an error status.
   * @internal
   */
  private async makeOpenRouterRequest(
    messages: LLMMessage[],
    model: string,
  ): Promise<OpenRouterResponse> {
    try {
      const res = await fetch("https://openrouter.ai/api/v1/chat/completions", {
        method: "POST",
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          "Content-Type": "application/json",
          // TODO: Make these configurable or remove/improve defaults
          "HTTP-Referer": "https://github.com/yourusername/tinyagent-ts",
          "X-Title": "TinyAgent-TS",
        },
        body: JSON.stringify({ model, messages }),
      });

      if (!res.ok) {
        let errorDetails: any = { message: "Failed to parse error response" };
        try {
          errorDetails = await res.json();
        } catch (parseError) {
          // Ignore parsing error, use default message
        }
        // TODO: Replace with LLMCommunicationError
        throw new Error(
          `OpenRouter API error: ${res.status} ${res.statusText}. Details: ${JSON.stringify(errorDetails)}`,
        );
      }

      // Explicitly type data after parsing JSON
      const data: any = await res.json();

      // Perform type checks before accessing properties
      if (
        !data ||
        typeof data !== "object" ||
        !data.choices ||
        !Array.isArray(data.choices) ||
        data.choices.length === 0 ||
        typeof data.choices[0]?.message?.content !== "string" // Check nested structure
      ) {
        console.error(
          "Invalid OpenRouter response structure:",
          JSON.stringify(data),
        );
        throw new Error(
          "Invalid response structure received from OpenRouter API",
        );
      }
      // Now it's safer to assert the type
      return data as OpenRouterResponse;
    } catch (error: unknown) {
      const message = error instanceof Error ? error.message : String(error);
      // TODO: Replace with LLMCommunicationError
      throw new Error(`Failed to call OpenRouter API: ${message}`);
    }
  }

  /**
   * Main entry point for running the agent.
   * It processes the input, interacts with the LLM, and potentially uses tools
   * to generate a final output.
   * @param input - The input to be processed by the agent.
   * @returns A promise that resolves with the agent's final output.
   */
  async run(input: I): Promise<O> {
    const modelName = this.getModelName();
    const tools = this.buildToolRegistry();
    // Simplified tool catalog for the prompt
    const toolCatalog = Object.values(tools)
      .map((t) => `- ${t.meta.name}: ${t.meta.description}`)
      .join("\n");

    const defaultPrompt =
      `You are an AI agent. You can use the following tools if needed:\n${toolCatalog}\n` +
      `To use a tool, respond ONLY with a single JSON object with "tool" and "args" keys, e.g., {"tool":"tool_name","args":{"param1":"value1"}}.\n` +
      `Otherwise, respond directly to the user.`;
    const systemPrompt = (this.customSystemPrompt ?? defaultPrompt).replace(
      "{{tools}}",
      toolCatalog,
    );

    const messages: LLMMessage[] = [
      { role: "system", content: systemPrompt },
      { role: "user", content: String(input) }, // Ensure input is stringified
    ];

    const responseBody = await this.makeOpenRouterRequest(messages, modelName);
    const reply = responseBody.choices[0]?.message?.content?.trim() ?? "";

    // Check if the LLM's reply is likely a JSON tool call
    if (
      reply.startsWith("{") &&
      reply.endsWith("}") &&
      reply.includes('"tool"')
    ) {
      try {
        const parsedJson = JSON.parse(reply);
        // Basic validation of the parsed JSON structure
        if (
          typeof parsedJson.tool !== "string" ||
          typeof parsedJson.args !== "object" ||
          parsedJson.args === null
        ) {
          throw new Error("Invalid tool call format in LLM response.");
        }
        const { tool: toolName, args: toolArgs } = parsedJson as {
          tool: string;
          args: Record<string, unknown>;
        };

        const selectedTool = tools[toolName];

        if (!selectedTool) {
          // TODO: Replace with ToolNotFoundError
          const errorMsg = `Error: LLM requested unknown tool "${toolName}". Available tools: ${Object.keys(tools).join(", ")}`;
          console.error(errorMsg);
          // Fallback: return an error message or the raw reply.
          // Consider sending this error back to the LLM in a future iteration.
          return `Error: Attempted to use an unknown tool: ${toolName}.` as unknown as O;
        }

        const toolResult = await selectedTool.call(toolArgs);
        // Send tool result back to LLM for a final, polished answer
        const finalAnswer = await this.followUpWithToolResult(
          toolResult,
          messages,
          reply,
        );
        return finalAnswer as O; // Assume finalAnswer conforms to O
      } catch (error: unknown) {
        const message = error instanceof Error ? error.message : String(error);
        console.error(
          "Tool execution or parsing error:",
          message,
          "\nLLM Reply:",
          reply,
        );
        // Fallback to returning the LLM's raw reply if tool execution fails
        return reply as unknown as O;
      }
    }
    // If not a tool call, return the LLM's reply directly
    return reply as unknown as O;
  }

  /**
   * Sends the result of a tool execution back to the LLM for a final response.
   * @param toolResult - The result obtained from executing a tool.
   * @param previousMessages - The history of messages in the current conversation.
   * @param llmToolCallResponse - The raw JSON string response from the LLM that initiated the tool call.
   * @returns A promise that resolves with the LLM's final, polished answer.
   * @internal
   */
  private async followUpWithToolResult(
    toolResult: unknown,
    previousMessages: LLMMessage[],
    llmToolCallResponse: string,
  ): Promise<string> {
    const messages: LLMMessage[] = [
      ...previousMessages,
      { role: "assistant", content: llmToolCallResponse }, // Include the LLM's decision to call the tool
      // NOTE: This format for providing tool results might need adjustment based on
      // the specific LLM provider's requirements (e.g., OpenAI uses a dedicated 'tool' role).
      // Using a simple TOOL_RESULT prefix for now.
      {
        role: "assistant",
        content: `TOOL_RESULT: ${JSON.stringify(toolResult)}`,
      },
      {
        role: "user",
        content:
          "Based on the tool result, please provide the final answer to my original query.",
      },
    ];

    const responseBody = await this.makeOpenRouterRequest(
      messages,
      this.getModelName(),
    );
    return responseBody.choices[0]?.message?.content?.trim() ?? "";
  }
}
